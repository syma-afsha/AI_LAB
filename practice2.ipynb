{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syma-afsha/AI_LAB/blob/main/practice2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hY88PZuGRlvk"
      },
      "source": [
        "# **Final Exam for Deep Network Development course. First part (mandatory)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "994g3vW4RpXr"
      },
      "source": [
        "This notebook contains the task to be solved in order to pass the exam.\n",
        "This is the first part of the exam, which is compolsury in order to get a grade. It contains a task similar to what you have worked on during the semester, which consists on implementing a network architecture and a function.\n",
        "\n",
        "Please note that, to **PASS** the Deep Network Development course you must **SUBMIT A SUCCESSFUL SOLUTION FOR THE FIRST PART**. If you **FAIL** the first part, you have the right to do the exam **ONE MORE TIME**. If you **FAIL AGAIN**, then unfortunately, you have failed the course. If you **PASS** the first part, then you get the weighted average of your quizzes and assignments as your final grade."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhN7-a6UiJjL"
      },
      "source": [
        "## Your information\n",
        "Please fill the next cell with your information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b0b9HjOdk2F"
      },
      "source": [
        "**Full Name**:\n",
        "\n",
        "**Neptun code:**\n",
        "\n",
        "**Date:** 16/01/2024 9AM-10AM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krzdzOL0Sejg"
      },
      "source": [
        "## Task Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjArI5jdUlLv"
      },
      "source": [
        "#### Your task is to implement a custom architecture layers and the forward functions.\n",
        "\n",
        "#### Afterwards, make sure to run the last cell code to check if your implementation is correct.\n",
        "\n",
        "#### This task should be **SOLVED IN 1 HOUR** and submitted to Canvas (download the .ipynb file). Please note that after 1 hour, the Canvas exam assignment will be closed and you cannot submit your solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8C4gtZ-Xqe6y"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9UZ9Gci_k3d",
        "outputId": "4230dd40-a94c-4d5f-c676-824783c6fe43"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwErjN2yP2O0"
      },
      "source": [
        "# Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1WbOZj1PkVa"
      },
      "source": [
        "Please right click the image and \"Open image in a new tab\" to view it better with zoom. Or download it from here: https://drive.google.com/file/d/1I4GNCq7OnANpLfknHFox52wnzlBnvmTj/view?usp=sharing\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "![](https://drive.google.com/uc?id=1I4GNCq7OnANpLfknHFox52wnzlBnvmTj)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCCilL8Pqzo-"
      },
      "source": [
        "# Text Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgWiUAU8slAj"
      },
      "outputs": [],
      "source": [
        "text = \"Brown fox jumps over the lazy dog\"\n",
        "\n",
        "# Simple preprocessing the text\n",
        "word_to_ix = {\"Brown\": 0, \"fox\": 1, \"jumps\": 2, \"over\": 3, \"the\": 4, \"lazy\": 5, \"dog\":6}\n",
        "input_tensor = torch.tensor(list(word_to_ix.values()), dtype=torch.long) # a tensor representing words by integers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxyvYiODqhqw",
        "outputId": "1a00e72a-0f91-4c30-8627-a9a4381914ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After embedding: torch.Size([7, 512])\n",
            "After view: torch.Size([1, 3584])\n",
            "After linear1: torch.Size([1, 2048])\n",
            "After linear2: torch.Size([1, 12288])\n"
          ]
        }
      ],
      "source": [
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TextEncoder, self).__init__()\n",
        "\n",
        "        #IMPLEMENT LAYERS\n",
        "        self.embedding=nn.Embedding(num_embeddings=7, embedding_dim=512)\n",
        "        self.linear1=nn.Linear(in_features=3584, out_features=2048)\n",
        "        self.linear2=nn.Linear(in_features=2048, out_features=12288)\n",
        "        self.relu=nn.ReLU()\n",
        "\n",
        "\n",
        "    def forward(self, text):\n",
        "        #IMPLEMENT FORWARD STEP\n",
        "        x=self.embedding(text)\n",
        "        print(f\"After embedding: {x.shape}\")\n",
        "        x=x.view(1,-1)\n",
        "        print(f\"After view: {x.shape}\")\n",
        "        x=self.linear1(x)\n",
        "        print(f\"After linear1: {x.shape}\")\n",
        "        x=self.relu(x)\n",
        "        x=self.linear2(x)\n",
        "        print(f\"After linear2: {x.shape}\")\n",
        "        x=self.relu(x)\n",
        "\n",
        "\n",
        "        # reshape embedding output to fit feed forward layer\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "encoder = TextEncoder()\n",
        "text_encoder=encoder(input_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RR2x8x4oAL9x"
      },
      "source": [
        "# Combined Text-Image Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5hBX97M1p2f"
      },
      "outputs": [],
      "source": [
        "class CombinedTextImageEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CombinedTextImageEncoder, self).__init__()\n",
        "        # IMPLEMENT LAYERS\n",
        "        self.conv1=nn.Conv2d(in_channels=3, out_channels=26, kernel_size=3, padding=1)\n",
        "        self.pool1=nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2=nn.Conv2d(in_channels=26, out_channels=40, kernel_size=3, stride=4,padding=1)\n",
        "        self.linear1=nn.Linear(in_features=2560, out_features=4096) #change out_features\n",
        "        self.relu=nn.ReLU()\n",
        "\n",
        "\n",
        "    def forward(self, encoded_text):\n",
        "      # IMPLEMENT FORWARD STEP\n",
        "      print(f\"Input shape: {encoded_text.shape}\")\n",
        "      # implement conv layers\n",
        "      x=self.conv1(encoded_text)\n",
        "      print(f\"After conv1: {x.shape}\")\n",
        "      x=self.pool1(x)\n",
        "      print(f\"After pool1: {x.shape}\")\n",
        "      x=self.conv2(x)\n",
        "      print(f\"After conv2: {x.shape}\")\n",
        "      batch_size, channels, height, width = x.shape\n",
        "      x=x.view(batch_size, -1)\n",
        "      x=self.linear1(x)\n",
        "      print(f\"After linear1: {x.shape}\")\n",
        "      x=self.relu(x)\n",
        "\n",
        "\n",
        "      #reshape output to fit Linear\n",
        "\n",
        "      #add batch number to Linears output\n",
        "\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwrAFKWMslAk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYbJkD0gEWaF"
      },
      "source": [
        "#Combined Text-Image Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZd9RNzkESmI"
      },
      "outputs": [],
      "source": [
        "class CombinedTextImageDecoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CombinedTextImageDecoder, self).__init__()\n",
        "        #IMPLEMENT LAYERS\n",
        "        self.gru=nn.GRU(input_size=4096, hidden_size=256, num_layers=3, batch_first=True)\n",
        "        self.conv1=nn.Conv2d(in_channels=4, out_channels=10, kernel_size=3, padding=0, stride=1)\n",
        "        self.pool1=nn.AvgPool2d(kernel_size=2, stride=1)\n",
        "        self.linear=nn.Linear(in_features=250, out_features=360)\n",
        "        self.relu=nn.ReLU()\n",
        "        self.convtrans1=nn.ConvTranspose2d(in_channels=20, out_channels=24, kernel_size=3, stride=3, padding=3)\n",
        "        self.convtrans2=nn.ConvTranspose2d(in_channels=24, out_channels=48, kernel_size=4, stride=6, padding=3)\n",
        "        self.convtrans3=nn.ConvTranspose2d(in_channels=48, out_channels=3, kernel_size=4, stride=4, padding=0) #out change\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, features):\n",
        "      print(f\"Input shape: {features.shape}\")\n",
        "      # IMPLEMENT FORWARD PASS FOLLOW INSTRUCTIONS\n",
        "      x1,_=self.gru(features)\n",
        "      print(f\"After GRU: {x1.shape}\")\n",
        "      x1=x1.view(1,-1)\n",
        "      print(f\"After view: {x1.shape}\")\n",
        "      x1=x1.unsqueeze(2).unsqueeze(3).view(1,4,8,8)\n",
        "      print(f\"After unsqueeze: {x1.shape}\")\n",
        "\n",
        "      x1=self.conv1(x1)\n",
        "      print(f\"After conv1: {x1.shape}\")\n",
        "      x=self.pool1(x1)\n",
        "      print(f\"After pool1: {x.shape}\")\n",
        "      x=x.view(1,-1)\n",
        "      print(f\"After view: {x.shape}\")\n",
        "      x=self.linear(x)\n",
        "      print(f\"After linear: {x.shape}\")\n",
        "      x=self.relu(x)\n",
        "      x=x.unsqueeze(2).unsqueeze(3).view(1,10,6,6)\n",
        "      print(f\"After unsqueeze: {x.shape}\")\n",
        "      x=torch.concat((x1,x),dim=1)\n",
        "      print(f\"After concatenation: {x.shape}\")\n",
        "      x=self.convtrans1(x)\n",
        "      print(f\"After convtrans1: {x.shape}\")\n",
        "      x=self.convtrans2(x)\n",
        "      print(f\"After convtrans2: {x.shape}\")\n",
        "      x=self.convtrans3(x)\n",
        "      print(f\"After convtrans3: {x.shape}\")\n",
        "      x=self.relu(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      #reshape GRU output to fit conv2D\n",
        "\n",
        "      #reshape Linear output to fit concatenation\n",
        "\n",
        "      return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqKSnz3BrCO3"
      },
      "source": [
        "# Combining everything"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvcrkVnbql8B",
        "outputId": "86b14028-8d39-40b3-b72a-f250251a5f69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After embedding: torch.Size([7, 512])\n",
            "After view: torch.Size([1, 3584])\n",
            "After linear1: torch.Size([1, 2048])\n",
            "After linear2: torch.Size([1, 12288])\n",
            "Text features: torch.Size([1, 12288])\n",
            "Input image: torch.Size([1, 3, 256, 256])\n",
            "Image reshape: torch.Size([1, 3, 64, 64])\n",
            "Text features reshape: torch.Size([1, 12288, 1, 1])\n",
            "Text features reshape: torch.Size([1, 3, 64, 64])\n",
            "add: torch.Size([1, 3, 64, 64])\n",
            "Input shape: torch.Size([1, 3, 64, 64])\n",
            "After conv1: torch.Size([1, 26, 64, 64])\n",
            "After pool1: torch.Size([1, 26, 32, 32])\n",
            "After conv2: torch.Size([1, 40, 8, 8])\n",
            "After linear1: torch.Size([1, 4096])\n",
            "Encoder: torch.Size([1, 4096])\n",
            "Unsqueeze: torch.Size([1, 1, 4096])\n",
            "Input shape: torch.Size([1, 1, 4096])\n",
            "After GRU: torch.Size([1, 1, 256])\n",
            "After view: torch.Size([1, 256])\n",
            "After unsqueeze: torch.Size([1, 4, 8, 8])\n",
            "After conv1: torch.Size([1, 10, 6, 6])\n",
            "After pool1: torch.Size([1, 10, 5, 5])\n",
            "After view: torch.Size([1, 250])\n",
            "After linear: torch.Size([1, 360])\n",
            "After unsqueeze: torch.Size([1, 10, 6, 6])\n",
            "After concatenation: torch.Size([1, 20, 6, 6])\n",
            "After convtrans1: torch.Size([1, 24, 12, 12])\n",
            "After convtrans2: torch.Size([1, 48, 64, 64])\n",
            "After convtrans3: torch.Size([1, 3, 256, 256])\n",
            "Decoder: torch.Size([1, 3, 256, 256])\n"
          ]
        }
      ],
      "source": [
        "class TextToImageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TextToImageModel, self).__init__()\n",
        "        self.textencoder = TextEncoder()\n",
        "        self.encoder = CombinedTextImageEncoder()\n",
        "        self.decoder = CombinedTextImageDecoder()\n",
        "        #write the layer with the interrogation mark\n",
        "\n",
        "    def forward(self, text):\n",
        "      text_features = self.textencoder(text)\n",
        "      print(f\"Text features: {text_features.shape}\")\n",
        "      #making random noise image\n",
        "      image = torch.rand(256, 256, 3).numpy()\n",
        "      image = (image * 255).astype(np.uint8)\n",
        "      image = np.rollaxis(image, 2, 0)\n",
        "      input_image = torch.from_numpy(image) # Convert image to torch tensor\n",
        "      input_image = torch.unsqueeze(input_image, dim=0) # Add a batch dimension of 1\n",
        "      print(f\"Input image: {input_image.shape}\")\n",
        "      image_reshape=input_image[:,:,0:64,0:64]\n",
        "      print(f\"Image reshape: {image_reshape.shape}\")\n",
        "      #reshape text_features to 4D tensor\n",
        "      text_feature1=text_features\n",
        "      text_feature1=text_feature1.unsqueeze(2).unsqueeze(3)\n",
        "      print(f\"Text features reshape: {text_feature1.shape}\")\n",
        "      text_feature1=text_feature1.view(1,3,64,64)\n",
        "      print(f\"Text features reshape: {text_feature1.shape}\")\n",
        "\n",
        "      x=image_reshape+text_feature1\n",
        "      print(f\"add: {x.shape}\")\n",
        "      x=self.encoder(x)\n",
        "      print(f\"Encoder: {x.shape}\")\n",
        "      x=x.unsqueeze(1)\n",
        "      print(f\"Unsqueeze: {x.shape}\")\n",
        "\n",
        "      x=self.decoder(x)\n",
        "      print(f\"Decoder: {x.shape}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      # do the block with the interogation mark\n",
        "\n",
        "\n",
        "      #reshape features to fit addition\n",
        "\n",
        "      #write other layers\n",
        "\n",
        "\n",
        "      return\n",
        "\n",
        "model = TextToImageModel()\n",
        "model(input_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9GyFNHRrJG2"
      },
      "source": [
        "# Trying Out the whole architecture DO NOT MODIFY THESE CELLS!!!!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlu8rC8Rp1xq"
      },
      "outputs": [],
      "source": [
        "text = \"Brown fox jumps over the lazy dog\"\n",
        "\n",
        "# Simple preprocessing the text\n",
        "word_to_ix = {\"Brown\": 0, \"fox\": 1, \"jumps\": 2, \"over\": 3, \"the\": 4, \"lazy\": 5, \"dog\":6}\n",
        "input_tensor = torch.tensor(list(word_to_ix.values()), dtype=torch.long) # a tensor representing words by integers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzcCRA1oK1Ty",
        "outputId": "dbd26f4b-6616-4ac2-94ef-cc5e76924426"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After embedding: torch.Size([7, 512])\n",
            "After view: torch.Size([1, 3584])\n",
            "After linear1: torch.Size([1, 2048])\n",
            "After linear2: torch.Size([1, 12288])\n",
            "Text features: torch.Size([1, 12288])\n",
            "Input image: torch.Size([1, 3, 256, 256])\n",
            "Image reshape: torch.Size([1, 3, 64, 64])\n",
            "Text features reshape: torch.Size([1, 12288, 1, 1])\n",
            "Text features reshape: torch.Size([1, 3, 64, 64])\n",
            "add: torch.Size([1, 3, 64, 64])\n",
            "Input shape: torch.Size([1, 3, 64, 64])\n",
            "After conv1: torch.Size([1, 26, 64, 64])\n",
            "After pool1: torch.Size([1, 26, 32, 32])\n",
            "After conv2: torch.Size([1, 40, 8, 8])\n",
            "After linear1: torch.Size([1, 4096])\n",
            "Encoder: torch.Size([1, 4096])\n",
            "Unsqueeze: torch.Size([1, 1, 4096])\n",
            "Input shape: torch.Size([1, 1, 4096])\n",
            "After GRU: torch.Size([1, 1, 256])\n",
            "After view: torch.Size([1, 256])\n",
            "After unsqueeze: torch.Size([1, 4, 8, 8])\n",
            "After conv1: torch.Size([1, 10, 6, 6])\n",
            "After pool1: torch.Size([1, 10, 5, 5])\n",
            "After view: torch.Size([1, 250])\n",
            "After linear: torch.Size([1, 360])\n",
            "After unsqueeze: torch.Size([1, 10, 6, 6])\n",
            "After concatenation: torch.Size([1, 20, 6, 6])\n",
            "After convtrans1: torch.Size([1, 24, 12, 12])\n",
            "After convtrans2: torch.Size([1, 48, 64, 64])\n",
            "After convtrans3: torch.Size([1, 3, 256, 256])\n",
            "Decoder: torch.Size([1, 3, 256, 256])\n"
          ]
        }
      ],
      "source": [
        "model = TextToImageModel()\n",
        "model = model.to(device)\n",
        "output = model(input_tensor)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}